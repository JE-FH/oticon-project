{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starter med at loaded den givne data ind. Her er dataen sepereret i lyd med musik og lyd uden musik. Det skal vi så sætte sammen i et array og lave en array som indeholder om hvilken type hver styk data er. Her kan vi bruge numpy siden hver fil er i .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = np.load(\"music_data.npy\")\n",
    "other_data = np.load(\"other_data.npy\")\n",
    "test_data = np.load(\"test_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data layoutet er her $10500$ 2d arrays af $30\\times79$ som repræsentere et log-mel spektogram udregnet fra den givne lyd sample. $x$ dimensionen er frekvens båndene mens $y$ er tids rammen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi sætte `music_data` og `other_data` sammen til et array og lave en array som indeholder hvilken type hver lyd sample er. Vi vil også gerne have noget test data så vi kan evaluere hvor godt netværket er uden at overfitte Det gør vi ved at efterlade 1000 fra `music_dat` og `other_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_train_data = music_data[0: -1000]\n",
    "other_train_data = other_data[0: -1000]\n",
    "train_data = (\n",
    "\tnp.concatenate((music_train_data, other_train_data)),\n",
    "\tnp.concatenate((\n",
    "\t\tnp.full((len(music_train_data)), (1)), \n",
    "\t\tnp.full((len(other_train_data)), (0))\n",
    "\t))\n",
    ")\t\n",
    "\n",
    "music_test_data = music_data[len(music_data) - 1000:None]\n",
    "other_test_data = other_data[len(other_data) - 1000:None]\n",
    "test_data = (\n",
    "\tnp.concatenate((music_test_data, other_test_data)),\n",
    "\tnp.concatenate((\n",
    "\t\tnp.full((len(music_test_data)), (1)),\n",
    "\t\tnp.full((len(other_test_data)), (0))\n",
    "\t))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu har vi lavet dataset til test og træning, nu skal vi sætte vores model op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 30, 79, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 77, 3)         30        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 38, 3)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 36, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 18, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 16, 64)         18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,594,335\n",
      "Trainable params: 1,594,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\ttf.keras.layers.Reshape((30, 79, 1), input_shape = (30, 79)),\n",
    "\ttf.keras.layers.Conv2D(3, (3, 3), activation = \"relu\"),\n",
    "\ttf.keras.layers.MaxPool2D(),\n",
    "\ttf.keras.layers.Conv2D(32, (3, 3), activation = \"relu\"),\n",
    "\ttf.keras.layers.MaxPool2D(),\n",
    "\ttf.keras.layers.Conv2D(64, (3, 3), activation = \"relu\"),\n",
    "\ttf.keras.layers.MaxPool2D(),\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dense(1024, activation = \"relu\"),\n",
    "\ttf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "\ttf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.build(input_shape=(30, 79))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu hvor vi har en model, så skal den bare trænes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4130155 ],\n",
       "       [0.36750004],\n",
       "       [0.44021606],\n",
       "       [0.44221535],\n",
       "       [0.39781332],\n",
       "       [0.341931  ],\n",
       "       [0.3665561 ],\n",
       "       [0.42156693],\n",
       "       [0.37554705],\n",
       "       [0.40658802]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(train_data[0][:10]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu laver vi en loss funktion, siden vi kun har et output så bruger vi binarycrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så træner vi modellen med dataen over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "594/594 [==============================] - 16s 26ms/step - loss: 0.2645 - accuracy: 0.8862\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 17s 29ms/step - loss: 0.1400 - accuracy: 0.9461\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 16s 26ms/step - loss: 0.0981 - accuracy: 0.9641\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 17s 28ms/step - loss: 0.0715 - accuracy: 0.9744\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 15s 25ms/step - loss: 0.0536 - accuracy: 0.9792\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 16s 27ms/step - loss: 0.0428 - accuracy: 0.9853\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 16s 27ms/step - loss: 0.0390 - accuracy: 0.9852\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 15s 25ms/step - loss: 0.0302 - accuracy: 0.9891\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 15s 24ms/step - loss: 0.0284 - accuracy: 0.9898\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 15s 25ms/step - loss: 0.0336 - accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data[0], train_data[1], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 5s 9ms/step - loss: 0.0154 - accuracy: 0.9951\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0819 - accuracy: 0.9705\n",
      "Training set accuracy 99.505% and loss: 1.535%\n",
      "Test set accuracy 97.050% and loss: 8.191%\n"
     ]
    }
   ],
   "source": [
    "train_evaluation = model.evaluate(train_data[0], train_data[1])\n",
    "test_evaluation = model.evaluate(test_data[0], test_data[1])\n",
    "print(\"Training set accuracy %.3f%% and loss: %.3f%%\" % \n",
    "\t(train_evaluation[1] * 100, train_evaluation[0] * 100))\n",
    "print(\"Test set accuracy %.3f%% and loss: %.3f%%\" % \n",
    "\t(test_evaluation[1] * 100, test_evaluation[0] * 100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86e4912f8ca300a9529e79ea444ccc65776bffefe7965c32e119ed643a92a698"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
