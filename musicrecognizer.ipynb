{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starter med at loaded den givne data ind. Her er dataen sepereret i lyd med musik og lyd uden musik. Det skal vi så sætte sammen i et array og lave en array som indeholder om hvilken type hver styk data er. Her kan vi bruge numpy siden hver fil er i .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = np.load(\"music_data.npy\")\n",
    "other_data = np.load(\"other_data.npy\")\n",
    "unknown_data = np.load(\"test_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data layoutet er her $10500$ 2d arrays af $30\\times79$ som repræsentere et log-mel spektogram udregnet fra den givne lyd sample. $x$ dimensionen er frekvens båndene mens $y$ er tids rammen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu skal vi sætte `music_data` og `other_data` sammen til et array og lave en array som indeholder hvilken type hver lyd sample er. Vi vil også gerne have noget test data så vi kan evaluere hvor godt netværket er uden at overfitte Det gør vi ved at efterlade 1000 fra `music_dat` og `other_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "music_train_data = music_data[0: -1000]\n",
    "other_train_data = other_data[0: -1000]\n",
    "train_data = (\n",
    "\tnp.concatenate((music_train_data, other_train_data)),\n",
    "\tnp.concatenate((\n",
    "\t\tnp.full((len(music_train_data), 2), (1, 0)), \n",
    "\t\tnp.full((len(other_train_data), 2), (0, 1))\n",
    "\t))\n",
    ")\n",
    "train_data = unison_shuffled_copies(train_data[0], train_data[1])\n",
    "\n",
    "music_test_data = music_data[len(music_data) - 1000:None]\n",
    "other_test_data = other_data[len(other_data) - 1000:None]\n",
    "test_data = (\n",
    "\tnp.concatenate((music_test_data, other_test_data)),\n",
    "\tnp.concatenate((\n",
    "\t\tnp.full((len(music_test_data), 2), (1, 0)),\n",
    "\t\tnp.full((len(other_test_data), 2), (0, 1))\n",
    "\t))\n",
    ")\n",
    "\n",
    "test_data = unison_shuffled_copies(test_data[0], test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to shuffle the data before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 30, 79, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 77, 3)         30        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 38, 3)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 36, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 18, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 11, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2560)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 5122      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,544\n",
      "Trainable params: 24,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 15:34:55.926798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-16 15:34:55.954758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-16 15:34:55.954774: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-16 15:34:55.955406: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\ttf.keras.layers.Reshape((30, 79, 1), input_shape = (30, 79)),\n",
    "\ttf.keras.layers.Conv2D(3, (3, 3), activation = \"relu\"),\n",
    "\ttf.keras.layers.MaxPool2D(pool_size = (1,2)),\n",
    "\ttf.keras.layers.Conv2D(32, (3, 3), activation = \"relu\"),\n",
    "\ttf.keras.layers.MaxPool2D(),\n",
    "\ttf.keras.layers.Conv2D(64, (3, 3), activation = \"relu\"),\n",
    "\ttf.keras.layers.MaxPool2D(),\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\t#tf.keras.layers.Dense(128, activation = \"relu\"),\n",
    "\t#tf.keras.layers.Dropout(.5),\n",
    "\ttf.keras.layers.Dense(2, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.build(input_shape=(30, 79))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu har vi lavet dataset til test og træning, nu skal vi sætte vores model op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu hvor vi har en model, så skal den bare trænes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.512843  , 0.48715693],\n",
       "       [0.9260722 , 0.07392789],\n",
       "       [0.8063127 , 0.19368735],\n",
       "       [0.96569985, 0.03430012],\n",
       "       [0.67638534, 0.3236147 ],\n",
       "       [0.8010059 , 0.19899414],\n",
       "       [0.5927658 , 0.40723425],\n",
       "       [0.67607313, 0.32392684],\n",
       "       [0.9420313 , 0.05796863],\n",
       "       [0.49783817, 0.50216186]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(train_data[0][:10]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu laver vi en loss funktion, siden vi kun har et output så bruger vi binarycrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så træner vi modellen med dataen over 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.3063 - accuracy: 0.8723\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.1228 - accuracy: 0.9563\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0846 - accuracy: 0.9701\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0671 - accuracy: 0.9766\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0476 - accuracy: 0.9844\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0463 - accuracy: 0.9844\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0340 - accuracy: 0.9879\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0326 - accuracy: 0.9892\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0278 - accuracy: 0.9906\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 10s 17ms/step - loss: 0.0251 - accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data[0], train_data[1], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594/594 [==============================] - 4s 6ms/step - loss: 0.0114 - accuracy: 0.9967\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0439 - accuracy: 0.9895\n",
      "Training set accuracy 99.674% and loss: 1.136%\n",
      "Test set accuracy 98.950% and loss: 4.394%\n"
     ]
    }
   ],
   "source": [
    "train_evaluation = model.evaluate(train_data[0], train_data[1])\n",
    "test_evaluation = model.evaluate(test_data[0], test_data[1])\n",
    "print(\"Training set accuracy %.3f%% and loss: %.3f%%\" % \n",
    "\t(train_evaluation[1] * 100, train_evaluation[0] * 100))\n",
    "print(\"Test set accuracy %.3f%% and loss: %.3f%%\" % \n",
    "\t(test_evaluation[1] * 100, test_evaluation[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 15:36:42.670311: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 479232000 exceeds 10% of free system memory.\n",
      "2022-04-16 15:36:42.785967: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 479232000 exceeds 10% of free system memory.\n",
      "2022-04-16 15:36:42.881715: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 479232000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "data = model(unknown_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"predictions.txt\", \"w\")\n",
    "\n",
    "first = True\n",
    "\n",
    "for prediction in data:\n",
    "\tif first:\n",
    "\t\tfirst = False\n",
    "\telse:\n",
    "\t\tf.write(\"\\n\")\n",
    "\tif prediction[0] > prediction[1]:\n",
    "\t\tf.write(\"1\")\n",
    "\telse:\n",
    "\t\tf.write(\"0\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lige at være sikker på modellen giver det vi forventer, så printer vi det lige her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 0.000\n",
      "0.000 1.000\n"
     ]
    }
   ],
   "source": [
    "pred = model(music_data[:1]).numpy()[0]\n",
    "print(\"%.3f %.3f\" % (pred[0], pred[1]))\n",
    "pred = model(other_data[:1]).numpy()[0]\n",
    "print(\"%.3f %.3f\" % (pred[0], pred[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86e4912f8ca300a9529e79ea444ccc65776bffefe7965c32e119ed643a92a698"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
